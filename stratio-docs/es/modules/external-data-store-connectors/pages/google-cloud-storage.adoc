= Google Cloud Storage (GCS)

Es un almacenamiento externo incluido en Google Cloud Platform. Se puede emplear para almacenar cualquier tipo de objeto que permita usos como el almacenamiento para aplicaciones de Internet, _backup_ y _restore_, recuperación de desastres, archivos de datos, _data lakes_ para análisis y almacenamiento híbrido en la nube.

== Matriz de soporte

|===
| Módulo | Versiones | Google Cloud Storage

| _Stratio Spark_
| ≥ 2.4.4-3.5.0 +
≥ 3.0.1-1.1.0
| OK

| _Stratio Virtualizer_
| ≥ 3.1.0
| OK

| _Stratio Data Governance_
| ≥ 1.8.1
| OK

| _Stratio Rocket_
| ≥ 2.1.0
| OK

| _Stratio Sparta_
| -
| -

| _Stratio Intelligence_
| -
| -

| _Stratio BDL_
| ≥ 1.8.1
| OK
|===

:note-caption: AVISO

NOTE: Los módulos sin versiones no han sido probados aún. Podrían estar soportados.

== _Stratio Spark_

=== Acceso al dato

_Stratio Spark_ incluye las dependencias del cliente de Hadoop para soportar Google Cloud Storage.

Soporta los formatos de archivo Parquet, CSV, JSON y ORC. Los formatos XML y Avro son compatibles con Spark, pero es necesario añadir algunas dependencias que no están incluidas en _Stratio Spark_. También se ha probado a crear particiones con archivos Parquet.

:note-caption: AVISO

NOTE: Solo se soporta una cuenta de Google en cada _job_ de Spark. Puedes configurar el acceso a varios _buckets_.

=== Autorización/gestión de secretos

Google Cloud Storage solo soporta el método de autorización *Google Service Account (OAuth 2.0)*. Para usarlo, necesitas almacenar los secretos en Vault. El equipo de Spark ha realizado un desarrollo en _Stratio Spark_ para almacenar estos secretos. Por favor, pídele al administrador del sistema que lo haga por ti:

* *Ruta de Vault*: `/v1/userland/passwords/s000001-spark-fw/s000001-gcs`.
* *Ejecutar en vCLI*: `put s000001-gcs {"user": "<private_key_id>", "pass": "<private_key>"}`.

:warning-caption: ADVERTENCIA

WARNING: Si estás usando _Stratio Spark_ en DC/OS sin soporte _multi-tenant_, no puedes establecer el nombre de la cuenta en la ruta de Vault. Tienes que establecer esta propiedad de Spark en su lugar: _"spark.mesos.dirverEnv.SPARK_SECURITY_ADLS2_ACCOUNT_NAME": "<account_name>"_.

Después, puedes lanzar el _job_ de Spark con las siguientes propiedades:

[source,json]
----
"spark.mesos.driverEnv.SPARK_SECURITY_GCS_ENABLE": "true",
"spark.mesos.driverEnv.SPARK_SECURITY_GCS_VAULT_PATH": "/v1/userland/passwords/s000001-spark-fw/s000001-gcs",
"spark.mesos.driverEnv.SPARK_SECURITY_GCS_PROJECT_ID": "<project_id>",
"spark.mesos.driverEnv.SPARK_SECURITY_GCS_SERVICE_ACCOUNT": "<service_account>"
----

* _project$$_$$id_: el ID del proyecto en Google Cloud. Se puede establecer en ``SPARK_SECURITY_GCS_PROJECT_ID``. Por ejemplo: ConnectorStorage.
* _service$$_$$account_: cuenta de servicio creada usando Google Cloud IAM y con acceso al _bucket_. Por ejemplo: connectors-service.

=== Guía de usuario

El primer paso es configurar las credenciales en Google Cloud y almacenar los secretos en Vault. Después, tienes que establecer las propiedades de Spark para descargar los secretos y configurar el acceso al almacén de datos. Una vez que se configura el almacén de datos, se accederá como si fuera un sistema de archivos normal (como HDFS). Revisa la <<_autorizacióngestión_de_secretos, sección de Administración de autorización/secretos>> para ver más instrucciones.

El URI utilizado por Google Cloud Storage para localizar los diferentes recursos es el siguiente:

[source,text]
----
gs://<bucket_name>/<path_to_file>
----

Ejemplo de código de Spark:

[source,scala]
----
val df = spark.read.
 parquet("gs://connectors-bucket/userdata1.parquet")

df.show()
----

Lanza el _job_ de Spark utilizando _Spark Dispatcher_. Necesitas establecer estas propiedades en el _job_ para descargar los secretos. El administrador del sistema aportará la ruta de Vault.

== _Stratio Virtualizer_

=== Acceso al dato

El acceso a los datos se realiza a través de _Stratio Spark_. Mira en <<_stratio_spark, la sección de _Stratio Spark_>> para más información.

=== Autorización/gestión de secretos

_Stratio Virtualizer_ usa los métodos de autorización soportados en _Stratio Spark_ (solo *Google Service Account (OAuth 2.0)*), y los secretos pueden almacenarse de forma segura en Vault. También es posible configurar las credenciales en texto plano utilizando variables de entorno, pero este método no se recomienda por razones de seguridad.

=== Guía de usuario

Para llevar a cabo una prueba, es necesario tener un proyecto de Google, crear una cuenta de almacenamiento y darle acceso a la cuenta de servicio.

Antes de nada, necesitas guardar las credenciales en Vault. Consulta la sección de <<_stratio_spark, _Stratio Spark_>> para más información.

El siguiente paso es desplegar _Stratio Virtualizer_ utilizando _Stratio Command Center_. Puedes encontrarlo en *_Environment_ → _External data stores_ → _GCS integration_*.

Una vez desplegado, es posible registrar la tabla en el catálogo y ejecutar consultas.

[source,text]
----
-- Read an existing parquet file
CREATE TABLE gcs_1 USING parquet OPTIONS (path 'gs://connectors-bucket/userdata1.parquet');
SELECT * from gcs_1;

-- Create a new parquet file in GCS with two columns and five rows
CREATE TABLE gcs_2 USING parquet OPTIONS (path 'gs://connectors-bucket/myfile.parquet') AS SELECT 1 AS id, 'Roque' AS name UNION SELECT 2 AS id, 'Miguel Angel' AS name UNION SELECT 3 AS id, 'Ivan' AS name UNION SELECT 4 AS id, 'Alberto' AS name UNION SELECT 5 AS id, 'Juan Miguel' AS name;
SELECT * from gcs_2;
----

== _Stratio Data Governance_

=== Acceso al dato

El agente de descubrimiento de HDFS tiene soporte para el descubrimiento de metadatos de Google Cloud Storage utilizando el cliente de Hadoop. Los formatos de archivo soportados son Parquet, Avro, ORC y CSV.

=== Autorización/gestión de secretos

El agente de descubrimiento actualmente soporta el método de autorización *Google Service Account (OAuth 2.0)*. Los secretos pueden almacenarse de forma segura en Vault. Consulta la sección de <<_stratio_spark,_Stratio Spark_>> para más información.

:tip-caption: CONSEJO

TIP: Es muy recomendable crear un usuario dedicado para el agente de descubrimiento con permisos limitados.

=== Guía de usuario

Requisitos previos:

* Una cuenta de almacenamiento dentro de un proyecto de Google.
* Una cuenta de servicio con acceso a la cuenta de almacenamiento.
* Una instalación de _Stratio Data Governance_.

El primer paso es crear los secretos en Vault. Estos no se crean automáticamente por el instalador de _Stratio Command Center_, por lo que debes pedirle al administrador del sistema que lo haga por ti. 

:tip-caption: CONSEJO

TIP: Se recomienda crear una nueva cuenta de Google para _Stratio Data Governance_ con permisos limitados.

Usa el descriptor de _Stratio Command Center_ para instalar el agente de descubrimiento de HDFS para Google Cloud Storage: _agent-cloud-default_.

Los campos más importantes a rellenar en la instalación son:

*General*

* _Backend_ de _Stratio Data Governance_ (PostgreSQL)
 ** _Host_: la instancia de PostgreSQL para guardar metadatos de Google Cloud Storage.
* Configuración externa
 ** HDFS a descubrir.
  *** _Data store type_: Google Cloud Storage.
  *** _Default FS_: sistema de ficheros por defecto. Por ejemplo: gs://connectors-bucket.
  *** _Init path_: la ruta desde la cual quieres descubrir metadatos de forma recursiva. Establece / si no estás seguro.
 ** Configuración de Google Cloud Storage
  *** _Authorization method_: debe ser OAUTH (_service account_). Los secretos se deben almacenar en Vault.
  *** _Google project ID_: nombre del proyecto de Google al que pertenece la cuenta de servicio. Por ejemplo: connectorstorage.
  *** _Google service account_: nombre de la cuenta de servicio. Por ejemplo: connectors-service.
 ** Identidad de servicio
  *** _Vault role_: se recomienda crear un nuevo rol para los agentes de descubrimiento. Por ejemplo: s000001-dg-agent.
 ** Red de Calico
  *** _Network name_: es necesario utilizar la red compartida de Stratio si el agente de descubrimiento está configurado para guardar los metadatos en Postgreseos.

*Ajustes*

* Ruta de secretos
 ** _Vault path_: ruta de Vault con las credenciales de autorización. Por ejemplo: s000001-dg-gcs-agent.
 ** _Instance name_: secreto de Vault con las credenciales de autorización. Por ejemplo: s000001-dg-gcs-agent.

Comprueba que el servicio despliega, es capaz de descargar el _driver_ y los secretos y comienza el proceso de descubrimiento. La primera vez puede tardar un tiempo.

Si el servicio funciona correctamente, puedes ver los metadatos descubiertos en las trazas:

[source,text]
----
Extract begins at: Fri Mar 27 09:56:05 CET 2020
NewOrUpdate 14 DataAssets begins at: Fri Mar 27 09:56:06 CET 2020
Delete 0 DataAssets begins at: Fri Mar 27 09:56:07 CET 2020
Synchronizing 14 and 0 Federated DataAssets begins at: Fri Mar 27 09:56:07 CET 2020
----

En la interfaz de usuario de _Stratio Data Governance_, puedes ver que se ha descubierto un nuevo almacén de datos y puedes examinar los metadatos. Todos los archivos, columnas y tipos de datos se han detectado correctamente.

image::external-gcs-connector-governance.png[]

El agente actualiza los metadatos periódicamente. Se puede realizar una prueba, por ejemplo, al cargar un nuevo archivo en Google Cloud Storage y esperando a que el agente detecte el cambio. Estos cambios se reflejan en la interfaz de usuario de _Stratio Data Governance_.

== _Stratio Rocket_

El acceso a los datos se realiza a través de _Stratio Spark_. Mira en la sección de <<_stratio_spark,_Stratio Spark_>> para ver más información.

Necesitas almacenar los secretos en Vault y establecer las variables de entorno. Luego, puedes utilizar todas las entradas y salidas de sistema de ficheros para leer y escribir datos en Google Cloud Storage. Funciona de la misma manera que HDFS.

== _Stratio GoSec_

Los almacenes de datos externos no están integrados en _Stratio GoSec_.

La autorización se configurará directamente en la base de datos cuando se cree el usuario para _Stratio Virtualizer_/_Stratio Spark_/_Stratio Data Governance_.

:tip-caption: CONSEJO

TIP: Es muy recomendable crear un usuario específico para cada aplicación con permisos limitados.

La mayoría de los componentes accederán al almacén de datos a través de _Stratio Virtualizer_. Esto te permite configurar diferentes políticas de autorización para cada usuario en _Stratio GoSec_.

Los secretos (usuario/contraseña) se pueden almacenar en Vault de forma segura. _Stratio Virtualizer_/_Stratio Spark_/_Stratio Data Governance_ tienen mecanismos para descargar los secretos y usarlos cuando sea necesario.

== Problemas conocidos

* Soporta los formatos de archivo Parquet, CSV JSON y ORC. Los formatos XML y Avro son compatibles con Spark, pero es necesario añadir algunas dependencias que no están incluidas en _Stratio Spark_. También se ha probado a crear particiones con archivos Parquet.
* Solo hay soporte para una cuenta de servicio, que debe tener los permisos necesarios para leer/escribir en los _buckets_.
