= Quick start guide

The purpose of this guide is to briefly explain how to integrate a https://www.databricks.com/product/databricks-sql/[Databricks workspace] into the _Stratio Generative AI Data Fabric_ platform.

== Prerequisites

You can find the prerequisites for installing the SSCC Databricks connector in the xref:databricks:operations-guide.adoc#_prerequisites[operations guide].

NOTE: Please note that the *Personal Access Token (PAT) authentication mode* will be used in this quick start guide.

== Discover your data

=== Discovery agent

To install a _Stratio Data Governance_ discovery agent for Databricks you must go to '_Stratio Command Center_' -> 'Deploy a Service' -> 'Connectors Data Warehouse' and select "Databricks Agent (External)".

When installing, you have to add the information corresponding to the database instance to be discovered, in addition to the URLs of the connectors repository where the necessary artifacts are stored and xref:databricks:operations-guide.adoc#create-secret[upload the access credentials to _Stratio KMS_].

The most important fields to fill in are:

* *_Root discovery path_*: path from which you want to discover the metadata recursively. Example: _/samples,/hive_metastore_.
* *_Custom Service URL_*: JDBC URL used to connect to Databricks. Example: _jdbc:databricks://adb-1234567890123456.7.azuredatabricks.net/-db-;httpPath=/sql/1.0/endpoints/abcdef1234567890_.
* *_Access credentials_*: name of the secret created in _Stratio KMS_. Example: _databricks-secret_.
* *_Data store driver location_*: URL where the artifact that will contain the JAR of the SSCC Databricks connector is located in the connectors repository. Example: `http://connectors.<tenant>-<namespace>/v1/api/artifact/sscc-hive-0.3_2.12-1.5.x.jar`.

TIP: More information on the other configuration parameters can be found in the xref:databricks:operations-guide.adoc#install-agent[operations guide].

Once the discovery process is completed, the data store will appear in the _Stratio Data Governance_ UI.

== Virtualize your data

=== Eureka agent

To use the BDL, you need to configure the Eureka agent with the Databricks connector. To do this, simply add the URL of the connectors repository of the `sscc-hive-0.3_2.12-1.5.x` artifact in the 'Customized deployment' -> 'Settings' -> `Additional jars` variable.

In the example being followed, the Eureka agent looks like this:

* _Additional jars_: `http://connectors.<tenant>-<namespace>/v1/api/artifact/sscc-hive-0.3_2.12-1.5.x.jar`.

=== _Stratio Virtualizer_

To use _Stratio Virtualizer_, the Databricks connector needs to be configured. To do so, you need to add the URL of the artifact in the 'Customized deployment' -> 'Environment' -> 'External datastores' -> `JDBC Drivers URL List` variable and also upload the access credentials to _Stratio KMS_.

In the example being followed, _Stratio Virtualizer_ looks like this:

* _JDBC Drivers URL List_: `http://connectors.<tenant>-<namespace>/v1/api/artifact/sscc-hive-0.3_2.12-1.5.x.jar`.

== Transform your data

=== _Stratio Rocket_

To use _Stratio Rocket_, the Databricks connector needs to be configured. To do this, you must add the URL of the artifact in the 'Customized deployment' -> 'Settings' -> 'Classpath' -> `Rocket extra jars` variable and also xref:databricks:operations-guide.adoc#create-secret[upload the access credentials for workflows and for _Stratio Rocket_ to _Stratio KMS_].

In the example being followed, _Stratio Rocket_ would look like this:

* _Rocket extra jars_: `http://connectors.<tenant>-<namespace>/v1/api/artifact/sscc-hive-0.3_2.12-1.5.x.jar`.

After applying these changes, you should be able to access the virtualized Databricks collection via SQL queries directly in the catalog or via a _Stratio Virtualizer_ input in a workflow.

TIP: More information on the other configuration parameters can be found in the xref:databricks:operations-guide.adoc#rocket-configuration[operations guide].

=== _Stratio Intelligence_

You'll need to configure _Stratio Intelligence_ as described in the xref:ROOT:quick-start-guide.adoc#_stratio_intelligence[general quick start guide] before integrating with the connector.

Once you've configured _Stratio Intelligence_, you have to add the Databricks connector and JDBC driver to the _artifacts/spark++_++jars_ directory and xref:databricks:operations-guide.adoc[upload the access credentials to _Stratio KMS_].

To upload the artifact, you have to download it to the _artifacts/spark++_++jars_ directory in the _Stratio Intelligence_ workspace by issuing a cURL request.

In this case, you could make the following request:

[source,bash]
----
curl http://connectors.<tenant>-<namespace>/v1/api/artifact/sscc-hive-0.3_2.12-1.5.x.jar --output sscc-hive-0.3_2.12-1.5.x.jar
----

Once this configuration is done, you'll be able to access the tables that have been virtualized from the external Databricks database.
