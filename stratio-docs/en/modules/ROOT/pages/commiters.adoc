= _Stratio Virtualizer_ integration with cloud storage

== Introduction

Apache Hadoop® (HDFS) is a file system that provides a subset of the behavior of a POSIX file system (or at least the implementation of the APIs and the POSIX file system model provided by Linux file systems). Is fully integrated with Apache Hadoop® and is extensively tested with each new release.

However, major cloud providers offer persistent data storage in object stores, such as Amazon S3, Azure Data Lake Storage Gen2 or Google Cloud Storage. These are not classic POSIX file systems, and if the connectors to these services are not configured correctly they may present some inconveniences in their integration with _Stratio Virtualizer_.

=== How does an object storage work?

An object storage is a data storage service typically accessed via HTTP/HTTPS.

Objects are stored by name: a string, possibly with "/" symbols in them, simulating a directory structure. Although there is no real directory hierarchy in them except for Azure Data Lake Storage Gen2 storage accounts with hierarchical namespace enabled.

To store hundreds of petabytes of data without any single point of failure, object stores replace the classic file system directory tree with a simpler object key => data model.

Object stores generally prioritizes availability; there is no single point of failure equivalent to Apache Hadoop® (HDFS) _NameNode_.

Object store connectors simulate being a file system with the same features and operations as Apache Hadoop® (HDFS). This is only a simulation and they have different features and sometimes fail for different reasons:

. *Consistency*: in general, object stores are eventually consistent. Changes to objects (creation, deletion and updates) may take time to be visible. In fact, there is no guarantee that a change will be immediately visible to the client that just made it. For example, a `test/data1.csv` object may be overwritten with a new data set, but when a `GET test/data1.csv` call is made shortly after the update, the original data is returned. Apache Hadoop® assumes that file systems are immediately consistent, that creation, updates, and deletions are immediately visible, and that the results of enumerating a directory are up-to-date with respect to the files within that directory.

. *Atomicity*: Apache Hadoop® assumes that directory rename operations are atomic, as are delete operations. Connectors for object stores implement them as operations on individual objects whose names match the directory prefix. As a result, changes are made on a file-by-file basis and are not atomic. If an operation fails mid-process, state of the object store reflects the partially completed operation.

. *Durability*: Apache Hadoop® assumes that _OutputStream_ implementations write data to their (persistent) storage in a `flush()` operation. Object storage implementations save all their written data to a local file which it then sends to the object store in the final shutdown operation. As a result, there is never any partial data for incomplete or failed operations. In addition, since the write process begins at the `close()` operation, that operation can take a time proportional to the amount of data to be loaded and inversely proportional to the network bandwidth.

. *Authorization*: Apache Hadoop® uses the _FileStatus_ class to represent file and directory metadata, including owner, group and permissions. Object stores may not have a viable way to retain this metadata.

Object stores with these features cannot be used as a direct replacement for Apache Hadoop® (HDFS). In terms of the above description, their implementations of the specified operations do not match those required. They are considered compatible with the Apache Hadoop® (HDFS) development community but not to the same extent as Apache Hadoop® (HDFS).

=== How does it affect _Stratio Virtualizer_?

_Stratio Virtualizer_ writes the result of the jobs to a file system. That is, each _Stratio Virtualizer_ task writes its output as a file. The system must produce consistent output even if:

* Some tasks are interrupted while in progress (e.g., deletion of a task or an executor runs out of memory) and can be retried on another executor.
* Several copies of the same task can be executed in parallel on different executors (a mechanism called speculation that helps with performance).

To solve this problem, a technique called *commit protocol* is used, which uses an intermediate (temporary) directory. At the end of the tasks, it lists the intermediate output directories and renames the files to their final location. This works well in the Apache Hadoop® Distributed File System (HDFS) because listing a directory produces consistent results and renaming a file is a quick "OR (1)" operation.

image::integration-dfs-spark.png[]

Therefore, in order for _Stratio Virtualizer_ to work securely with an object storage, two conditions must be met:

* Reading after writing must be totally consistent. In other words, consistency must be immediate.
* The directory renaming operation must be atomic.

Apache Hadoop® provides connectors for some of the major object storage services, although in some cases the requirements necessary to ensure consistency are violated. As stated above, they cannot be used as a direct replacement for a distributed file system such as Apache Hadoop® (HDFS), *except when explicitly stated by the connector developer itself*.

NOTE: As of 2021, Amazon S3, Google Cloud Storage and Microsoft (Azure Storage, ADLS Gen1, ADLS Gen2) object stores *are consistent*. This means that as soon as a file is written/updated, other processes can display, view and open it, and the latest version will be retrieved. However, *the renaming problem exists in some cases*, and to solve it there are different solutions that will be discussed on a case-by-case basis*.

Current situation:

[cols="1,1,1,1"]
|===
| Storage | Connector | Directory renaming safety | Renaming performance

| Amazon S3
| s3a
| Unsafe
| O(data)

| Azure Storage
| wasb
| Safe
| O(files)

| Azure Data Lake Storage Gen2
| abfs
| Safe
| O(1)

| Google Cloud Storage
| gs
| Mixed *
| O(files)
|===

https://spark.apache.org/docs/latest/cloud-integration.html#recommended-settings-for-writing-to-object-stores[Integration with cloud infrastructures - Apache Spark™ 3.3.1 documentation]

* https://issues.apache.org/jira/browse/MAPREDUCE-7341[([#MAPREDUCE-7341\] Add a task-manifest output commiter for Azure and Google Cloud Storage - ASF JIRA)]

IMPORTANT: If you are working with table-like formats such as Delta.io, Apache Iceberg or Apache Hudi, the use of committer in Amazon S3 is not relevant as these table formats handle the commit process differently.

== Amazon S3

=== The "commit" problem in Amazon S3

_Stratio Virtualizer_ uses the `FileOutputCommitter` class to manage the promotion of files created in a single attempt of a task to the final result of a query. This is done to handle task and job failures and to support speculative execution. It does this by listing directories and renaming their contents at the final destination when tasks and then jobs are confirmed.

This has some key requirements of the underlying file system:

. When it lists a directory, it sees all files that have been created in it and no files that are not in it (i.e., that have been deleted).
. When it renames a directory, it is an atomic transaction. No other process in the cluster can rename a file or directory to the same path. If the renaming fails for some reason, the data is either in the original location or in the destination, in which case the renaming was successful.

In the past, the Amazon S3 object store and the `s3a://` file system client could not meet these requirements, but S3A is currently consistent. Internally, Amazon S3 still performs renaming by copying files and then deleting the originals. This can fail in the middle and there is nothing to prevent another process in the cluster from trying to rename at the same time.

As a result:

* If a rename fails, the data is left in an unknown state.
* If more than one process attempts to commit the job simultaneously, the output directory may contain the results of both processes as it is not an exclusive operation.

IMPORTANT: Using the "classic" `FileOutputCommitter` to submit work to Amazon S3 runs the risk of losing or damaging the generated data*.

=== Using "committers" in Amazon S3

To address the issues in the job commit phase, there is explicit support in the _hadoop-aws_ module for submitting jobs to Amazon S3 via the S3A file system client.

For a safe and high performance job output, it is necessary to use a committer explicitly written to work with Amazon S3, treating it as an object store with special features.

Starting with Apache Hadoop® 3.1, the S3A file system incorporates classes designed to integrate with the Apache Hadoop® and _Stratio Virtualizer_ jobs commit protocols, classes that interact with the S3A file system to reliably commit the job to Amazon S3.

The key concept to be aware of is Amazon S3's "multipart loading" mechanism. This allows an Amazon S3 client to write data in multiple `HTTP POST` requests only completing the write operation with a final `POST` to complete the upload. This multi-part loading mechanism is already used automatically when writing large amounts of data to Amazon S3.

S3A's committers make explicit use of this multipart loading mechanism:

. Individual tasks in a job write their data to Amazon S3 as `POST` operations within multi-part loads but don't issue a final `POST` to complete the load.
. Multi-part loads are confirmed in the commit process of the job.

In addition to the default Apache Hadoop® committer (`FileOutputCommitter`), there are two different types of S3A committer called _staging_ and _magic_. These vary primarily in how data is written during task execution, how pending commit information is passed to the jobs manager, and how conflicts with existing files are resolved.

==== Staging committer

Developed by Netflix and given to the community. It writes data to _Stratio Virtualizer_'s cluster-level shared file system (HDFS or NFS), so tasks are written to URLs with "file://" schemas. When a task is committed, its files are listed and uploaded to Amazon S3 as incomplete multipart uploads. The information needed to complete the uploads is stored locally, where it is confirmed via the standard confirmation algorithm "v1". When the job is committed, it reads the pending write lists from its target directory from the local job and completes those uploads.

Canceling a task is simple: the local directory deletes its temporary data. Canceling a job is accomplished by reading the lists of pending writes from the local temporary job directory and aborting those loads. For added security, all pending multipart writes to the target directory are aborted.

The _staging committer_ comes in two slightly different forms in terms of conflict resolution. These are:

* _Directory_: the entire data directory tree is written or overwritten, as usual.
* _Partitioned_: special handling of partitioned directory trees, such as the format `YEAR=2017/MONTH=09/DAY=19`. Conflict resolution is limited to partitions that are updated. This mode is designed to allow jobs to update a partitioned directory tree to restrict conflict resolution to only those partition directories that contain new data. It is designed to be used only with _Stratio Virtualizer_.

When a task is confirmed, the data is loaded into the target directory. A policy on how to react if data already exists in the target can be set using the `fs.s3a.committer.staging.conflict-mode` configuration parameter. (https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/committers.html)

CAUTION: The use of these committer with _Stratio Intelligence_ is not recommended due to permissions issues described below. Alternatively, you can use the _magic committer_. xref:#_use_with_stratio_intelligence[(See section 2.6 - consideration 3)].

==== _Magic committer_

The _magic committer_ completes the _Stratio Virtualizer_ jobs on the target file system (Amazon S3's bucket). It writes to specific paths in a parent directory. When the output stream is closed, the information needed to complete the write is saved in the directory used by _magic_. If the task is canceled, it lists the pending writes and cancels them.

Compared to _staging_, _magic committer_ offers faster writing times: the output is uploaded to Amazon S3 as it is written avoiding local write and subsequent upload.

Initially, this committer required installing a Dynamodb database to enable an Amazon S3 client mechanism called S3Guard ("protecting" it against inconsistent results) but, as of December 2020, https://aws.amazon.com/en/about-aws/whats-new/2020/12/amazon-s3-now-delivers-strong-read-after-write-consistency-automatically-for-all-applications/[Amazon S3 offers full robust read-after-write consistency globally], so it is no longer necessary to install S3Guard.

=== Which committer should I use in each case?

[cols="1,1"]
|===
| Committer | Use case

| FileOutputCommitter
| When working with Amazon S3, there is no guarantee of atomic operation in the file renaming used by this committer on commit writes. Therefore, its use with Amazon S3 should be avoided.

| Staging - Directory
| You can use this committer when not working with too much data and the data is not partitioned, but consider the need for local space while committing and uploading completed jobs to Amazon S3. *Not recommended for use with _Stratio Intelligence_ xref:#_use_with_stratio_intelligence[(see section 2.6 - consideration 3)]*.

| Staging - Partitioned
| This committer is suitable if the use case works with partitioned data and usually writes to certain partitions. However, the need for local space should be considered while committing and uploading completed jobs to Amazon S3. *Not recommended for use with _Stratio Intelligence_ xref:#_uso_con_stratio_intelligence[(see section 2.6 - consideration 3)]*.

| Magic
| In general, this committer is recommended for Amazon S3 as it can work well with large amounts of data and its performance is much higher. As an advantage, it doesn't need local storage space in the _Stratio Virtualizer_ cluster because it works directly on Amazon S3.
* It is no longer necessary to use s3Guard since, by default, https://aws.amazon.com/es/about-aws/whats-new/2020/12/amazon-s3-now-delivers-strong-read-after-write-consistency-automatically-for-all-applications/[Amazon S3 is consistent].
|===

=== Committer configuration

To configure a committer, it will be necessary to set the following parameters in the _Stratio Virtualizer_ session:

[cols="1,1"]
|===
| Parameter | Value

| `fs.s3a.path.style.access`
| true

| `mapreduce.outputcommitter.factory.scheme.s3a`
| org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory

| `fs.s3a.committer.name`
| It will be the committer to use:

    - directory
    - partitioned
    - magic

| `spark.sql.sources.commitProtocolClass` (only Parquet)
| org.apache.spark.internal.io.cloud.PathOutputCommitProtocol

| `spark.sql.parquet.output.committer.class` (only Parquet)
| org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter

| `fs.s3a.committer.staging.conflict-mode`
| Only applies to the "directory" and "partitioned" committers and will be how conflicts should be resolved:

- append: adds new files to the existing target directory.
- fail: fails if the target directory exists.
- replace: deletes existing files before uploading new ones.

| `fs.s3a.committer.staging.tmp.path`
|Only applies to the "directory" and "partitioned" committers and will be the path used for the temporary files in local.
|===

=== How to verify that the committer is working?

When a job write is finished, _Stratio Virtualizer_ generates a file called `_SUCCESS` in the destination indicating that it has been successfully executed. This file has empty content when using the default _Stratio Virtualizer_ committer. However, when using some of those listed above (_staging_ or _magic_), it contains a number of metadata describing the committer used among others.

=== Use with _Stratio Intelligence_

The integration of the use of specific committers with _Stratio Intelligence_ (Universe 13.0) has been tested. While it can be used correctly, the following considerations need to be applied and taken into account to avoid possible errors or inconsistencies:

. When working with Parquet files, you need to add a dependency that is not currently integrated in the product. To do this, from the kernels configuration of _Stratio Intelligence_ the following configuration must be added:
+
[source]
----
--conf spark.jars.packages="org.apache.spark:spark-hadoop-cloud_2.12:3.1.1.3.1.7270.0-253" --conf spark.jars.repositories="https://repository.cloudera.com/artifactory/cloudera-repos/"
----

. Once the kernel is configured and restarted, you can use the Amazon S3 committer. To do this, you must configure a number of parameters in the _Stratio Virtualizer_ session. In the case of the _magic committer_:
+
[source]
--
spark._jsc.hadoopConfiguration().set("fs.s3a.path.style.access","true")
spark._jsc.hadoopConfiguration().set("mapreduce.outputcommitter.factory.scheme.s3a","org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory")
spark._jsc.hadoopConfiguration().set("fs.s3a.committer.name","magic")
spark._jsc.hadoopConfiguration().set("spark.sql.sources.commitProtocolClass","org.apache.spark.internal.io.cloud.PathOutputCommitProtocol")
spark._jsc.hadoopConfiguration().set("spark.sql.parquet.output.committer.class","org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter")
--

. An important issue when using _Stratio Intelligence_ with Amazon S3 committer of type _staging_ is that `SPARK_USER` must match the _Stratio Intelligence_ user because there are folder/file movement tasks (as described in the committer operation) that are triggered by _Stratio Virtualizer_. Therefore, it is *not recommended to use _staging_-type committers (_directory_ or _partitioned_) with _Stratio Intelligence_. Failing that, _magic_ should be used when the value set in `SPARK_USER` and the _Stratio Intelligence_ user writing to Amazon S3 cannot be the same*.

== Azure Data Lake Storage Gen2 and Azure Blob

There are no specific committers as with Amazon S3. However, the Apache Hadoop® connector for Azure has the following features for both Azure Data Lake Storage Gen2 and Azure Blob usage:

* They support reading and writing data stored in an Azure Blob Storage account.
* They provide a consistent view of storage across all clients. In other words, the reading is consistent.
* They can read data written through the _wasb_ connector.
* They present a hierarchical view of the file system by implementing the standard Apache Hadoop® interface.
* They can act as a data source or target in Apache Hadoop® MapReduce, Apache Hive™ and _Stratio Virtualizer_.
* They are tested to scale on Linux and Windows by Microsoft.
* They can be used as a replacement for Apache Hadoop® (HDFS) on Apache Hadoop® clusters deployed on Azure infrastructure.

As with all Azure storage services, they provide a fully consistent view with complete create, read, update and delete consistency for data and metadata.

https://hadoop.apache.org/docs/current/hadoop-azure/abfs.html[Apache Hadoop® Azure support: ABFS - Azure Data Lake Storage Gen2]
https://hadoop.apache.org/docs/stable/hadoop-azure/index.html[Apache Hadoop® Azure support: Azure Blob Storage]

[NOTE]
====
By default, directory renaming behavior is not atomic for Azure Blob. However, you can configure the `fs.azure.atomic.rename.dir` parameter that allows you to set which paths will be treated atomically in rename operations (you can receive a comma-separated list of paths).

In the case of https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction#designed-for-enterprise-big-data-analytics[Azure Data Lake Storage Gen2], renaming is atomic by default.
====

== Google Cloud Storage

Google Cloud Storage (GCS) offers an https://github.com/GoogleCloudDataproc/hadoop-connectors[open source connector] that is consistent on read after a write. However, in the case of renaming, the operation is atomic with respect to file, not being atomic with respect to directory.

To have a correct operation with _Stratio Virtualizer_, the connector offers the ability to set a lock during the write to Google Cloud Storage. This parameter is called: `fs.gs.cooperative.locking.enable` and must be set to `true` if you want to have an exclusive lock on the destination path while a job is running.

As described by https://cloud.google.com/blog/products/data-analytics/new-release-of-cloud-storage-connector-for-hadoop-improving-performance-throughput-and-more?utm_source=pocket_reader[the Google Cloud Storage documentation], the _Stratio Virtualizer_ job (using the library) acquires a lock that has to be renewed from time to time (for the duration of the operation). If for some reason the job ends unexpectedly, Google Cloud Storage automatically removes the lock when the block is not renewed, so as not to leave the resource blocked indefinitely. If the job ends correctly, the connector itself will cancel the lock. The lock refresh time can be configured using the parameter https://github.com/GoogleCloudPlatform/bigdata-interop/blob/v2.0.0/gcs/CONFIGURATION.md#cooperative-locking-feature-configuration[`fs.gs.cooperative.locking.expiration.timeout.ms`], which has a default value of 120s.

image::integration-dfs-explained-gcs.png[]

== Conclusions

As can be seen from the analysis performed, due to the operation of the "commit protocol" algorithm used by _Stratio Virtualizer_, *working with Cloud storage systems such as Amazon S3, Azure Blob Storage, Azure Data Lake Storage Gen2 or Google Cloud Storage requires certain considerations to ensure that no incidents arise that could compromise data consistency*.

It may happen that, during the execution of _Stratio Virtualizer_ jobs that target object storage, it does not guarantee immediate read consistency or an atomic directory rename operation. Thus, if a job fails in the middle of execution, data could be left inconsistently in the destination and there would be no mechanism to prevent a second job from reading that data.

To solve this problem, mechanisms have been implemented in the connectors used by _Stratio Virtualizer_. These mechanisms *must be explicitly enabled to ensure consistent writes in all situations*. Otherwise, the default _Stratio Virtualizer_ mechanism (_FileOutputCommitter_) is used, which will not be consistent in all circumstances.

[cols="1,1,1"]
|===
| Storage |Connector | Is it safe to use with _Stratio Virtualizer_?

| Amazon S3
| s3a
| Yes, as long as you use some of the _staging_ or _magic_ committers with the configuration indicated in section 2.4.
*The recommended committer is _magic_. The use of the _staging_ type committer xref:#_use_with_stratio_intelligence[(review section 2.6 - consideration 3)] should be discarded*.

| Azure Blob Storage
| wasb
| Yes, by configuring the connector with the `fs.azure.atomic.rename.dir` parameter as described in section 3.

| Azure Data Lake Storage Gen2
| abfs
| Yes, without applying any specific configuration as Azure Data Lake Storage Gen2 already provides all the necessary mechanisms.

| Google Cloud Storage
| gs
| Yes, using the open source connector provided by Google and setting the `fs.gs.cooperative.locking.enable` parameter to `true`, as indicated in section 4.
|===

== References

* https://spark.apache.org/docs/3.1.1/cloud-integration.html
* https://www.databricks.com/blog/2017/05/31/transactional-writes-cloud-storage.html
* https://issues.apache.org/jira/browse/MAPREDUCE-7341
* https://stackoverflow.com/questions/66933229/writing-to-google-cloud-storage-with-v2-algorithm-safe
* http://www.openkb.info/2019/04/what-is-difference-between.html
* https://cloud.google.com/storage/docs/consistency
* https://hadoop.apache.org/docs/stable/hadoop-azure/index.html
* https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction#designed-for-enterprise-big-data-analytics
* https://hadoop.apache.org/docs/current/hadoop-azure/abfs.html
* https://aws.amazon.com/es/about-aws/whats-new/2020/12/amazon-s3-now-delivers-strong-read-after-write-consistency-automatically-for-all-applications/
* https://aws.amazon.com/es/about-aws/whats-new/2020/12/amazon-s3-now-delivers-strong-read-after-write-consistency-automatically-for-all-applications/
* http://www.openkb.info/2019/04/what-is-difference-between.html
* https://cloud.google.com/blog/products/data-analytics/new-release-of-cloud-storage-connector-for-hadoop-improving-performance-throughput-and-more?utm_source=pocket_reader
