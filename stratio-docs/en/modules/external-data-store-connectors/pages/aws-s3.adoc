= AWS S3

S3 is an external storage included in _Amazon Web Services (AWS)_ that can be employed to store any type of object which allows uses like storage for Internet applications, backup and recovery, disaster recovery, data archives, data lakes for analytics, and hybrid cloud storage.

== Known issues

* _Stratio Spark_ only allows storing a single secret for S3. This means that the same Spark Job can not connect securely to two AWS S3 accounts at the same time.
* Supports PARQUET, CSV, JSON, and ORC file formats. The XML and AVRO formats are supported by Spark, but it is necessary to add some dependencies that are not included in _Stratio Spark_. Partitioning with PARQUET files has also been tested.
* _Stratio Command Center_ descriptor includes support for this data store since version 2.22.0. For previous versions, you have to deploy a regular _Stratio Crossdata_ and then change some environment variables.

== Support matrix

|===
| Module | Versions | AWS S3

| _Stratio Spark_
| ≥ 2.4.4-3.1.0
| OK

| _Stratio Crossdata_
| ≥ 2.21.0
| OK

| _Stratio Data Governance_
| ≥ 1.5.0
| OK

| _Stratio Rocket_
| ≥ 1.1.0
| OK

| _Stratio Sparta_
| ≥ 2.15
| OK

| _Stratio Intelligence_
| -
| -

| _Stratio BDL_
| ≥ 1.6.0
| OK
|===

[box type="info"]Modules without versions are not tested yet. They might be supported.[/box]

== _Stratio Spark_

=== Data access

_Stratio Spark_ includes the Hadoop client dependencies to support AWS S3.

It supports PARQUET, CSV, JSON, and ORC file formats. The XML and AVRO formats are supported by Spark, but it is necessary to add some dependencies that are not included in _Stratio Spark_. Partitioning with PARQUET files has also been tested.

=== Authorization/Secrets management

S3 supports several authorization methods:

* *Access key and secret key*: the Spark team has made development in _Stratio Spark_ to store the secrets in Vault.
* *IAM Role (assume role)*: this is the recommended method because it allows for more specific permission control and the creation of temporary credentials. It is supported and the Spark team has made development in _Stratio Spark_ to store the secrets in Vault.

==== Access key and secret key

To use this method, you need to store the secrets in Vault. Please ask the system administrator to do it for you.

*Vault path*: `/v1/userland/passwords/s000001-aws-s3`
*Run in vCLI*: `put s000001-aws-s3 {"user": "<access_key>", "pass": "<secret_key>"}`

Then, you can launch the Spark Job with the following properties:

[source,json]
----
"spark.mesos.driverEnv.SPARK_SECURITY_S3_ENABLE": "true",
"spark.mesos.driverEnv.SPARK_SECURITY_S3_SECRETS_VAULT_PATH": "/v1/userland/passwords/s000001-aws-s3"
----

[box type="info"]_Stratio Spark_ only allows storing a single secret for S3. This means that the same Spark Job cannot connect securely to two AWS S3 accounts at the same time.[/box]

==== IAM Role (assume role)

This is the recommended method because it allows for more specific permission control and the creation of temporary credentials.

The first step is to change the AWS account configuration to use the IAM Role. In the https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html[AWS documentation] you can find useful information. This is the summary of the steps:

1) Create a new role in IAM.
2) Modify the user policy with assume role permissions.
3) Create a policy associated with that role.
4) Modify the Trust relationships JSON inside that role.
5) Modify the bucket policy to read/write.

Then, you need to store the secrets in Vault. Please ask the system administrator to do it for you.

*Vault path*: `/v1/userland/passwords/s000001-aws-s3`
*Run in vCLI*: `put s000001-aws-s3 {"user": "<access_key>", "pass": "<secret_key>"}`

After that, you can launch the Spark Job with the following properties:

[source,json]
----
"spark.mesos.driverEnv.SPARK_SECURITY_S3_ENABLE": "true",
"spark.mesos.driverEnv.SPARK_SECURITY_S3_SECRETS_VAULT_PATH": "/v1/userland/passwords/s000001-aws-s3",
"spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider",
"spark.hadoop.fs.s3a.assumed.role.arn":"<assumed_role>",
"spark.hadoop.fs.s3a.assumed.role.sts.endpoint": "https://sts.eu-west-1.amazonaws.com",
"spark.hadoop.fs.s3a.assumed.role.sts.endpoint.region": "eu-west-1"
----

* assumed_role: this is the role we created in AWS. Eg: arn:aws:iam::769605326598:role/Role-S3-Bucket-Spark

[box type="info"]_Stratio Spark_ only allows storing a single secret for S3. This means that the same Spark Job cannot connect securely to two AWS S3 accounts at the same time.[/box]

=== User guide

The first step is to choose one of the authorization methods, set up the credentials in AWS, and store the secrets in Vault. Then, you have to set the Spark properties to download the secrets and configure the data store access. Once the data store is configured, it will be accessed as if it were a normal file system (like HDFS). See the <<AuthorizationSecrets_management,Authorization/Secrets management section>> for instructions.

The URI used by AWS S3 to locate the different resources is as follows:

[source,text]
----
s3a://<bucket_name>/<path_to_file>
----

Example Spark code:

[source,scala]
----
val df = spark.read.
 parquet("s3a://demo-spark/sample-data/userdata1.parquet")

df.show()
----

Launch the Spark job using _Spark Dispatcher_. You need to set these properties in the job to download the secrets. The Vault path will be provided by the system administrator.

== _Stratio Crossdata_

=== Data access

Access to data is done through _Stratio Spark_. See the <<Stratio_Spark,_Stratio Spark_ section>> for more information.

=== Authorization/Secrets management

_Stratio Crossdata_ uses the same authorization methods supported in _Stratio Spark_.

The recommended method is "`IAM Role`" and the secrets can be safely stored in Vault. It is also possible to configure the credentials in plain text using environment variables, but this method is not recommended for security reasons.

=== User guide

In order to carry out a test, it is necessary to have an AWS account, create an S3 bucket, and configure the access credentials.

First of all, you need to save the credentials in Vault. See the Spark/Secrets section for instructions.

The next step is to deploy _Stratio Crossdata_ using _Stratio Command Center_. You can find them in *Environment → External data stores → AWS - S3 integration*.

[box type="info"]The _Stratio Command Center_ descriptor is available since version 2.22.0. For previous versions, you have to talk with the system administrator.[/box]

Once deployed, it is possible to register the table in the catalog and execute queries.

[source,text]
----
-- Read an existing parquet file
CREATE TABLE s3_1 USING parquet OPTIONS (path 's3a://demo-spark/sample-data/parquet/userdata1.parquet');
SELECT * from s3_1;

-- Create a new parquet file in S3 with two columns and five rows.
CREATE TABLE s3_2 USING parquet OPTIONS (path 's3a://demo-spark/my_file.parquet') AS SELECT 1 AS id, 'Roque' AS name UNION SELECT 2 AS id, 'Miguel Angel' AS name UNION SELECT 3 AS id, 'Ivan' AS name UNION SELECT 4 AS id, 'Alberto' AS name UNION SELECT 5 AS id, 'Juan Miguel' AS name;
SELECT * from s3_2;
----

== _Stratio Data Governance_

=== Data access

The HDFS discovery agent has support for discovery S3 metadata using the Hadoop S3 client. Supported file formats are PARQUET and AVRO.

=== Authorization/Secrets management

The Discovery agent currently supports "`access key and secret key`" and "`IAM Role`" authorization methods. Secrets can be safely stored in Vault. See the <<Stratio_Spark,_Stratio Spark_ section>> for more information.

It is highly recommended to create a dedicated user for the discovery agent with limited permissions.

=== User guide

Prerequisites:

* An S3 account with access to an S3 bucket filesystem.
* A _Stratio Data Governance_ installation.

The first step is to create the secrets in Vault. These secrets are not created automatically by the _Stratio Command Center_ installer. You have to ask the system administrator to do it for you. It's highly recommended to create a new user in AWS for _Stratio Data Governance_ with limited permissions. See the Spark secrets section for instructions.

*Vault path*: `/v1/userland/passwords/s000001-dg-s3-agent.s000001.marathon.mesos/s000001-dg-s3-agent.s000001.marathon.mesos`
*Run in vCLI*: `put <vault_path> {"user": "<access_key>", "pass": "<secret_key>"}`

Use the  _Stratio Command Center_ descriptor to install the HDFS discovery agent for S3: _agent-cloud-default_.

The most important fields to fill in the installation are:

*General*

* Backend _Stratio Data Governance_ (PostgreSQL)
 ** Host: PostgreSQL instance to save S3 metadata
* External configuration
 ** Cloud server to discover
  *** Data store type: S3.
  *** Default FS: default file system. Example: s3a://demo-spark.
  *** Init path: the path from which you want to discover the metadata recursively. Set/if you are not sure.
 ** S3 configuration
  *** S3 Authorization method: can be ASSUMED ROLE or ACCESS KEY. In both cases, the secrets must be stored in Vault.
  *** S3 Assumed Role Endpoint Region: only for Assumed Role authentication. Example: eu-west-1.
  *** S3 Assumed Role ARN: only for Assumed Role authentication. Example: arn:aws:iam::769605326598:role/Role-S3-Bucket-Spark.
 ** Service identity
  *** Vault role: it's recommended to create a new role for discovery agents. Eg: s000001-dg-agent.
 ** Calico network
  *** Network name: it's necessary to use the stratio-shared network if the discovery agent is configured to save the metadata in Postgreseos.

*Settings*

* Secrets path
 ** Vault path: Vault path with the authorization credentials. Eg: s000001-dg-s3-agent.
 ** Instance name: Vault secret with the authorization credentials. Eg: s000001-dg-s3-agent.

Check that the service deploys, is able to download the driver and secrets, and the discovery process begins. The first time may take a while.

If the service works correctly, you can see the discovered metadata in the traces:

[source,text]
----
Extract begins at: Fri Mar 27 09:56:05 CET 2020
NewOrUpdate 14 DataAssets begins at: Fri Mar 27 09:56:06 CET 2020
Delete 0 DataAssets begins at: Fri Mar 27 09:56:07 CET 2020
Synchronizing 14 and 0 Federated DataAssets begins at: Fri Mar 27 09:56:07 CET 2020
----

In the _Stratio Data Governance_ UI you can see that a new data store has been discovered, and you can browse the metadata. All files, columns, and data types have been detected correctly.

image::../attachments/external-awss3-connector-governance.png[]

The agent updates the metadata periodically. A test can be performed, for example, uploading a new file into S3 and waiting for the agent to detect the change. These changes are reflected in the _Stratio Data Governance_ UI.

== _Stratio Rocket_/_Stratio Sparta_

Access to data is done through _Stratio Spark_. See the <<Stratio_Spark,_Stratio Spark_ section>> for more information.

_Stratio Command Center_ descriptor includes support for this data store. You can find AWS S3 fields in the *General → External configuration → S3 configuration enabled* section.

The most important fields to fill in the installation are:

*General*

* External configuration
 ** S3 configuration enabled: enable AWS S3 support.
 ** Credentials Vault path: Vault path with the secrets. This is provided by the system administrator.

== _Stratio GoSec_

External data stores are not integrated into _Stratio GoSec_.

The authorization will be configured directly in the database when the user is created for _Stratio Crossdata_/_Stratio Spark_/_Stratio Data Governance_. It is recommended to create a specific user for each application with limited permissions.

Most modules will access the data store through _Stratio Crossdata_. This allows you to configure different authorization policies for each user in _Stratio GoSec_.

Secrets (user/password) can be stored in Vault safely. _Stratio Crossdata_/_Stratio Spark_/_Stratio Data Governance_ have mechanisms to download the secrets and use them when necessary.
