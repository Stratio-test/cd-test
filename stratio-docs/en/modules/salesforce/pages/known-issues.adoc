= Known issues

* The SSCC Salesforce connector only allows writing to Salesforce in _append_ and _upsert_ mode. For both modes, it is necessary to perform in a previous step a transformation asset removing those system columns created and auto-generated by Salesforce. Columns such as:
** Id
** LastModifiedDate
** IsDeleted
** etc.

* *Partial data* retrievals on large data sets using the https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/api_asynch_introduction_bulk_api.htm[Bulk API] option:
** The Salesforce Bulk API has a 5 minute limit for full data retrieval. If it fails to retrieve the data in that time, it will return the partial result obtained up to that point. To avoid this behavior, the API gives the option to separate the data retrieval on the Salesforce server side at https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/asynch_api_code_curl_walkthrough_pk_chunking.htm[chunks].
** To enable this option in _Stratio Rocket_, you need to add the following custom property:
*** *`pkChunking`*: `true`.
** You can also configure the size of those chunks with the following property:
*** *`chunkSize`*: <chunk_size> (default value: 100,000).
+
image::salesforce-with-chunks-enabled.png[]

* Define the maximum number of characters allowed for a value. If using the Bulk API you are extracting data with a large length, you will get the following error: "Length of parsed input (4097) exceeds the maximum number of characters defined in your parser settings (4096)". To avoid this, you can set the maximum allowed size for a data:
** *`maxCharsPerColumn`* (optional): <max_column_size> (default value: 4096). Available as of version 2.1.0.

* Error when writing a certain number of records:
+
[source,bash]
----
Failed to parse CSV. Exceed number of records : 10002. Number of records should be less than or equal to 10001;
Timeout : It took longer than 5 minutes to process this batch. Trying again later.
----
+
The Salesforce API has a limit per batches when writing. To control the number of elements per batch you need to include the following parameter:
+
** *`maxRecordsPerBatch`*: <number_records_per_batch>.
+
image::salesforce-rocket-max-records.png[]

* Planned quality rules on dictionary tables cannot be executed if the table has no data since the _Stratio Spark_ data source is not able to infer columns correctly in that case.
