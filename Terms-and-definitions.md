* **Anti Money Laundering (AML)**: it is a term mainly used in the financial and legal industries to describe the legal controls that require financial institutions and other regulated entities to prevent, detect, and report money laundering activities.
* **Application Programming Interface (API)**: is a set of subroutine definitions, communication protocols, and tools for building software.
* **Batch processing**: is where the processing happens of blocks of data that have already been stored over a period of time. In technological terminology, a batch process is a sequence of commands which is executed once and then terminates. In *big data* terminology, the batch process refers to a lengthy data processing pipeline applied to static historical data; a process that can take a long time to terminate, often scheduled for offline execution. Often the term appears together with its counterpart: stream processing which is a fast sequence of operations applied in real-time to live data streams.
* **Data lake**: it consists of a storage repository of all data in an enterprise ranging from raw data to transformed data which is used for various tasks, including reporting, visualization, analytics, and ML. Data is stored in its natural format, including structured, semi-structured, unstructured, and even binary data.
* **Data quality**: the art or science of keeping information from organizations consistent, accurate, complete, current, unique, and, most importantly, valid for the purpose it was created.
* **Data store**: is a repository for persistently storing and managing collections of data.
* **Functions builder**: an integrated helper tool to build custom functions by using more than 50 predefined operators.
* **Foreign key**: in a table, it is a set of one or more columns that refers to the primary key in another table.
* **Hadoop Distributed File System (HDFS)**: is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences between them are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. It provides high throughput access to application data and is suitable for applications that have large data sets. Also, HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project and is now an Apache Hadoop subproject.
* **Microservice**: an architectural style that structures an application as a collection of services highly maintainable and testable, loosely coupled, independently deployable, and organized around business capabilities.
* **Model**: it defines a typical configuration for a service. The models are created to describe frequent deployment styles or even versions between the releases. A single service configuration can have multiple models.
* **Primary key**: in a table, it consists of one or more columns whose data contained within are used to uniquely identify each row in the table.
* **Representational State Transfer (REST)**: it is a software architectural style that provides interoperability among systems on the Internet. RESTful systems allow accessing and manipulating textual representations of web resources by using a uniform and predefined set of stateless operations.
* **Rule**: the heart of the Rules Engine where you specify conditions (if ‘a’ then ‘b’).
* **Scheme**: it refers to the organization of data as a blueprint of how the database is constructed (divided into database tables in the case of relational databases).
* **Stratio Augmented Data Fabric**: a complete product to build the next generation of digital applications, putting the data in the center of your organization.
* **Structured Query Language (SQL)**: is a standard for obtaining, manipulating, and managing data in Relational Database Management Systems. It is common to use the term “SQL Database” to refer to a database with SQL capabilities.
* **Unique key**: in a table, it can be used to ensure rows are unique within the database. The main difference with a primary key comes from it’s intended use.
* **Workflow**: it defines a unique data pipeline that is going to be executed. It is composed of one to n inputs, several transformations, and one or several outputs, i.e: read from CSV, clean some data, store in HDFS. It can be versioned, tagged, and grouped. There are two types: streaming or batch.
